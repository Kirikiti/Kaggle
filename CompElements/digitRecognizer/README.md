# ğŸ§  Reconocimiento de dÃ­gitos/letras manuscritas  
GuÃ­a rÃ¡pida para [competir en Kaggle](https://www.kaggle.com/competitions/digit-recognizer/)

## 1. ğŸ“¦ PreparaciÃ³n de los datos
- Convertir imÃ¡genes a escala de grises.
- Normalizar valores a rango [0, 1].
- Redimensionar (28Ã—28 o 32Ã—32 segÃºn el dataset).
- One-hot encoding de las etiquetas (Aâ€“Z o 0â€“9).
- Dividir en train/validaciÃ³n.

## 2. ğŸ”§ Modelos recomendados

### ğŸŒ€ A. CNN bÃ¡sica (rÃ¡pida y efectiva)
Funciona muy bien para letras y dÃ­gitos simples.

Arquitectura tÃ­pica:
- Conv2D (32 filtros, 3Ã—3) + ReLU  
- Conv2D (32 filtros, 3Ã—3) + ReLU  
- MaxPooling  
- Dropout  
- Conv2D (64 filtros, 3Ã—3)  
- MaxPooling  
- Flatten  
- Dense (128) + ReLU  
- Dense (n_clases) + Softmax

### ğŸš€ B. Modelos mÃ¡s potentes
- **ResNet18 / ResNet34**  
- **EfficientNet-B0**  
- **MobileNetV2** (rÃ¡pido y ligero)

Estos suelen mejorar el score si el dataset es grande.

### ğŸ§ª C. Ensembles
Combinar varios modelos (promedio de predicciones) suele mejorar el leaderboard.

---

## 3. ğŸ¨ Data Augmentation
Muy Ãºtil para letras manuscritas:

- Rotaciones pequeÃ±as (Â±10Â°)  
- Zoom  
- Shear  
- Shift horizontal/vertical  
- PequeÃ±o ruido gaussiano  

Evita transformaciones que deformen demasiado la letra.

---

## 4. ğŸ Estrategia para competir
1. Entrenar una CNN bÃ¡sica para tener baseline.  
2. Probar modelos preentrenados (transfer learning).  
3. Ajustar augmentation.  
4. Hacer ensemble de los mejores modelos.  
5. Afinar el threshold o promediar logits para mejorar el score final.

---

## 5. ğŸ“Š MÃ©tricas
- **Accuracy** si es clasificaciÃ³n pura.  
- **F1-score** si las clases estÃ¡n desbalanceadas.  

---

## 6. ğŸ§© CÃ³digo base (Keras)

```python
import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])
```

---

# ğŸ” Modelos clÃ¡sicos para reconocer dÃ­gitos/letras manuscritas  
*(sin usar redes neuronales)*

Estos algoritmos funcionan sorprendentemente bien en datasets como MNIST, EMNIST o competiciones de letras individuales.

---

## ğŸ¥‡ 1. SVM (Support Vector Machines)
- Uno de los mejores modelos clÃ¡sicos para imÃ¡genes pequeÃ±as.
- Con kernel RBF suele superar el 97â€“98% en MNIST.
- Requiere normalizar y a veces reducir dimensionalidad.

**Ventajas:** muy preciso, robusto.  
**Desventajas:** lento con muchos datos.

---

## ğŸ¥ˆ 2. Random Forest
- Funciona bien sin mucha ingenierÃ­a.
- Captura relaciones no lineales.
- RÃ¡pido de entrenar.

**Ventajas:** fÃ¡cil de usar, buen baseline.  
**Desventajas:** no llega al rendimiento de SVM.

---

## ğŸ¥‰ 3. Gradient Boosting / XGBoost / LightGBM
- Suelen superar a Random Forest.
- Muy buenos con features derivados de imÃ¡genes (HOG, PCA, etc.).

**Ventajas:** excelente rendimiento.  
**Desventajas:** requieren tuning.

---

## ğŸ”¢ 4. KNN (k-Nearest Neighbors)
- Sorprendentemente fuerte en MNIST si se usa PCA o reducciÃ³n de dimensiÃ³n.
- Muy simple: no entrena, solo compara.

**Ventajas:** fÃ¡cil, buen baseline.  
**Desventajas:** lento al predecir.

---

## ğŸ§© 5. Logistic Regression (multiclase)
- Funciona mejor de lo que parece si las imÃ¡genes estÃ¡n bien normalizadas.
- Buen punto de partida.

**Ventajas:** rÃ¡pido, interpretable.  
**Desventajas:** limitado para patrones complejos.

---

## ğŸ§± 6. Naive Bayes
- Solo Ãºtil como baseline muy bÃ¡sico.
- Funciona mejor con datos binarizados.

---

## ğŸ¨ 7. HOG + Modelo clÃ¡sico
Una combinaciÃ³n muy potente:
- Extraer **HOG (Histogram of Oriented Gradients)** de cada imagen.
- Entrenar un **SVM**, **Random Forest** o **XGBoost** encima.

Esto era el estÃ¡ndar antes de las CNN y sigue siendo competitivo.

---

## â­ RecomendaciÃ³n prÃ¡ctica
Para una competiciÃ³n de letras manuscritas:

1. **HOG + SVM (RBF)** â†’ suele ser el mejor modelo clÃ¡sico.  
2. **HOG + XGBoost** â†’ muy competitivo.  
3. **PCA + SVM** â†’ rÃ¡pido y preciso.  
4. **KNN + PCA** â†’ baseline sorprendentemente fuerte.

---

Si quieres, te preparo:
- un **notebook completo** con HOG + SVM,  
- una **comparativa de todos estos modelos**,  
- o una **pipeline optimizada** para Kaggle.

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
