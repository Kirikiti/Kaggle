# ğŸ§© Cambios entre el modelo MoE original y la versiÃ³n compatible con ONNX

Este documento describe de forma clara y estructurada las diferencias entre:

- El modelo Mixture of Experts (MoE) original, basado en selecciÃ³n dinÃ¡mica de expertos.
- La versiÃ³n ONNXâ€‘compatible, diseÃ±ada para permitir exportaciÃ³n a ONNX sin romper el entrenamiento.

La estructura sigue el formato solicitado:  
**Concepto cambiado â†’ Antes â†’ Ahora â†’ ExplicaciÃ³n.**

---

## ğŸ”§ Concepto cambiado 1 â€” SelecciÃ³n dinÃ¡mica de expertos

### âŒ Antes (modelo original)
```python
expert_out = torch.stack([
    self.experts[e](x[j].unsqueeze(0))
    for j, e in enumerate(expert_idx)
])
```

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
```python
expert_outputs = []
for expert in self.experts:
    expert_outputs.append(expert(x))

expert_outputs = torch.stack(expert_outputs, dim=1)
```

### ğŸ“ ExplicaciÃ³n
El modelo original seleccionaba expertos segÃºn Ã­ndices calculados en tiempo de ejecuciÃ³n.  
ONNX no soporta selecciÃ³n dinÃ¡mica de mÃ³dulos, bucles dependientes de datos ni list comprehensions con tensores.  
La versiÃ³n ONNX ejecuta todos los expertos siempre, permitiendo un grafo estÃ¡tico exportable.

---

## ğŸ”§ Concepto cambiado 2 â€” Routing Topâ€‘K duro â†’ mezcla suave

### âŒ Antes (modelo original)
```python
topk_vals, topk_idx = torch.topk(logits, self.k, dim=1)
gate_scores = torch.softmax(topk_vals, dim=1)
```

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
```python
gate_logits = self.w_gating(x)
gate_scores = torch.softmax(gate_logits, dim=1)
```

### ğŸ“ ExplicaciÃ³n
El modelo original usaba Topâ€‘K routing, donde solo los mejores expertos participaban.  
ONNX no puede representar esta selecciÃ³n condicional.  
La versiÃ³n ONNX usa una mezcla suave: todos los expertos reciben un peso, aunque muchos sean casi cero.

---

## ğŸ”§ Concepto cambiado 3 â€” Bucle dependiente del batch â†’ bucle fijo

### âŒ Antes (modelo original)
```python
for j, e in enumerate(expert_idx):
    ...
```

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
```python
for expert in self.experts:
    expert_outputs.append(expert(x))
```

### ğŸ“ ExplicaciÃ³n
El bucle original dependÃ­a del contenido del batch, lo cual ONNX no puede convertir.  
El nuevo bucle es fijo y recorre Ãºnicamente los expertos, garantizando compatibilidad.

---

## ğŸ”§ Concepto cambiado 4 â€” CombinaciÃ³n manual â†’ combinaciÃ³n vectorizada

### âŒ Antes (modelo original)
```python
outputs.append(gate_scores[:, i].unsqueeze(1) * expert_out.squeeze(1))
return sum(outputs)
```

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
```python
gate_scores = gate_scores.unsqueeze(-1)
output = torch.sum(expert_outputs * gate_scores, dim=1)
```

### ğŸ“ ExplicaciÃ³n
La combinaciÃ³n original usaba listas dinÃ¡micas y sumas iterativas.  
La versiÃ³n ONNX usa operaciones vectorizadas, que ONNX puede representar sin problemas.

---

## ğŸ”§ Concepto cambiado 5 â€” EliminaciÃ³n de TopKGate

### âŒ Antes (modelo original)
```python
self.gate = TopKGate(input_dim, num_experts, k)
gate_scores, topk_idx = self.gate(x)
```

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
```python
self.w_gating = nn.Linear(input_dim, num_experts)
gate_scores = torch.softmax(self.w_gating(x), dim=1)
```

### ğŸ“ ExplicaciÃ³n
TopKGate depende de torch.topk, que ONNX no puede convertir cuando se usa para seleccionar mÃ³dulos.  
La versiÃ³n ONNX usa un gating lineal estÃ¡ndar.

---

## ğŸ”§ Concepto cambiado 6 â€” Arquitectura MoE simplificada pero equivalente

### âŒ Antes (modelo original)
- Routing duro (solo topâ€‘k expertos)  
- SelecciÃ³n dinÃ¡mica  
- EjecuciÃ³n parcial de expertos  

### âœ”ï¸ Ahora (modelo ONNXâ€‘compatible)
- Routing suave (todos los expertos)  
- Sin selecciÃ³n dinÃ¡mica  
- Grafo estÃ¡tico exportable  

### ğŸ“ ExplicaciÃ³n
La versiÃ³n ONNX mantiene la esencia del MoE, pero elimina toda operaciÃ³n dinÃ¡mica que ONNX no puede convertir.  
El gating sigue controlando la importancia de cada experto, aunque ahora todos se evalÃºan siempre.

---

## ğŸ“Œ Resumen final

| Componente | Original | ONNXâ€‘compatible |
|-----------|----------|-----------------|
| SelecciÃ³n de expertos | DinÃ¡mica (topâ€‘k) | EstÃ¡tica (todos los expertos) |
| Routing | Hard routing | Soft routing |
| Bucle | Dependiente de datos | Fijo |
| Exportable a ONNX | âŒ No | âœ”ï¸ SÃ­ |
| Velocidad | MÃ¡s rÃ¡pido | MÃ¡s lento |
| Calidad del modelo | Excelente | Muy similar |

---

## ğŸ§  ConclusiÃ³n

La versiÃ³n ONNXâ€‘compatible mantiene la esencia del MoE, pero elimina toda operaciÃ³n dinÃ¡mica que ONNX no puede convertir.  
El resultado es un modelo entrenable, estable, exportable y funcionalmente equivalente en la prÃ¡ctica.
